# ğŸ¤– Sakila Data AI Agent ğŸ“Š

A powerful AI-powered chatbot that helps you analyze and query the Sakila database (a sample MySQL database representing a DVD rental store) using natural language. Built with OpenAI's GPT-4o-mini model, Python, and Streamlit.

## ğŸŒŸ Features

- ğŸ—£ï¸ **Natural Language Querying**: Ask questions in plain English and get SQL-powered answers
- ğŸ¤– **AI-Powered Agent**: Uses OpenAI's GPT-4o-mini with function calling capabilities
- ğŸ¨ **Interactive UI**: Beautiful, modern chat interface built with Streamlit
- ğŸ” **Query Transparency**: See the generated SQL queries in expandable sections
- ğŸ“Š **Data Visualization**: Results displayed as interactive pandas DataFrames
- ğŸŒ **Flexible Database Support**: Works with both local MySQL and cloud PostgreSQL databases
- ğŸ§  **Schema-Aware**: Complete Sakila database schema embedded in the AI's context
- ğŸ”§ **Tool Calling**: Advanced implementation using OpenAI's function calling API

## ğŸ› ï¸ Prerequisites

- **Python 3.8+**
- **Database Options:**
  - Local: MySQL with Sakila database installed, OR
  - Cloud: PostgreSQL database (e.g., Neon, AWS RDS, Google Cloud SQL)
- **OpenAI API key** (get one at [platform.openai.com](https://platform.openai.com))
- **Required Python packages** (see Installation section)

## ğŸ“¦ Installation & Setup

### 1. Fork & Clone the Repository

```bash
git clone https://github.com/yourusername/sakila-data-ai-agent.git
cd sakila-data-ai-agent
```

### 2. Install Required Packages

```bash
pip install -r requirements.txt
```

Or install core packages directly:
```bash
pip install openai streamlit pymysql sqlalchemy python-dotenv pandas psycopg
```

### 3. Set Up Your Database

#### Option A: Local MySQL Database

1. Install MySQL if you haven't already
2. Import the Sakila database using the SQL scripts provided:

```bash
# Run scripts in order:
mysql -u root -p < sakila-sql-scripts/sakila_schema_00_create_schema.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_01_base_data.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_02_location_data.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_03_film_data.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_04_staff_store_customer.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_05a_rental_data_part1.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_05b_rental_data_part2.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_06a_payment_data_part1.sql
mysql -u root -p < sakila-sql-scripts/sakila_data_06b_payment_data_part2.sql
```

#### Option B: Cloud Database

See `Cloud_DB_Guide.ipynb` for detailed instructions on setting up a cloud PostgreSQL database with the Sakila data.

### 4. Configure Environment Variables

Create a `.env` file in the project root:

```bash
# OpenAI API Configuration
OPENAI_API_KEY=your_api_key_here

# Local MySQL Configuration (if using local database)
DB_HOST=localhost
DB_USER=root
DB_PASSWORD=your_database_password
DB_NAME=sakila
DB_PORT=3306

# Cloud PostgreSQL Configuration (if using cloud database)
POSTGRES_HOST=your_cloud_host
POSTGRES_USER=your_username
POSTGRES_PASSWORD=your_password
POSTGRES_DB=sakila
POSTGRES_PORT=5432
```

## ğŸš€ Usage

### Start the Application

```bash
streamlit run app_solution.py
```

Or if you're building from scratch:
```bash
streamlit run app.py
```

### Using the Chatbot

1. Open your web browser to the provided local URL (typically `http://localhost:8501`)
2. Start asking questions about the Sakila database in natural language
3. View the AI's responses along with the generated SQL queries
4. Explore the data displayed in interactive tables

## ğŸ’¡ Example Queries

Here are some questions you can ask:

**Basic Queries:**
- "How many movies are in the database?"
- "Show me all the actors"
- "What are all the film categories?"

**Complex Analytics:**
- "What are the top 10 most rented movies?"
- "Show me the top 5 customers by total rental count"
- "What's the average rental duration by film category?"
- "Which stores have the highest revenue?"

**Business Insights:**
- "Who are the most profitable customers?"
- "What are the most popular film categories by rental count?"
- "Show me monthly revenue trends"
- "Which actors appear in the most films?"

## ğŸ—„ï¸ Project Structure

```
sakila-data-ai-agent/
â”œâ”€â”€ ai/                              # AI agent modules
â”‚   â”œâ”€â”€ agent.py                     # Core agent implementation with tool calling
â”‚   â”œâ”€â”€ tools.py                     # Database querying tools (local & cloud)
â”‚   â”œâ”€â”€ prompts.py                   # System prompts with schema information
â”‚   â”œâ”€â”€ sakila_schema.py             # Complete Sakila database schema
â”‚   â””â”€â”€ utils.py                     # Database connection utilities
â”œâ”€â”€ sakila-sql-scripts/              # SQL scripts for database setup
â”‚   â”œâ”€â”€ sakila_schema_00_create_schema.sql
â”‚   â”œâ”€â”€ sakila_data_01_base_data.sql
â”‚   â”œâ”€â”€ sakila_data_02_location_data.sql
â”‚   â”œâ”€â”€ sakila_data_03_film_data.sql
â”‚   â”œâ”€â”€ sakila_data_04_staff_store_customer.sql
â”‚   â”œâ”€â”€ sakila_data_05a_rental_data_part1.sql
â”‚   â”œâ”€â”€ sakila_data_05b_rental_data_part2.sql
â”‚   â”œâ”€â”€ sakila_data_06a_payment_data_part1.sql
â”‚   â””â”€â”€ sakila_data_06b_payment_data_part2.sql
â”œâ”€â”€ app.py                           # Starter template (build your own)
â”œâ”€â”€ app_solution.py                  # Complete reference implementation
â”œâ”€â”€ Developer_Guide.ipynb            # Comprehensive tutorial notebook
â”œâ”€â”€ Cloud_DB_Guide.ipynb             # Cloud database setup guide
â”œâ”€â”€ Structured_Output_Guide.ipynb   # Advanced: Using structured outputs
â”œâ”€â”€ requirements.txt                 # Python package dependencies
â”œâ”€â”€ .env                             # Environment variables (create this)
â””â”€â”€ README.md                        # This file
```

## ğŸ”§ Technical Details

### Architecture

The project implements a modern AI agent architecture using:

1. **OpenAI Function Calling**: The agent uses GPT-4o-mini with function calling to determine when to query the database
2. **Tool Implementation**: Custom Python functions that execute SQL queries and return formatted results
3. **Schema-Aware Prompting**: The complete Sakila database schema is embedded in the system prompt
4. **Stateful Conversation**: Streamlit session state maintains conversation history
5. **Dynamic SQL Generation**: The LLM generates SQL queries based on natural language input

### Key Technologies

- **OpenAI API (GPT-4o-mini)**: Natural language understanding and SQL generation
- **Streamlit**: Interactive web application framework
- **SQLAlchemy**: Database ORM and connection management
- **PyMySQL**: MySQL database adapter
- **Psycopg**: PostgreSQL database adapter
- **Pandas**: Data manipulation and display
- **Python-dotenv**: Environment variable management

### How It Works

1. User enters a natural language question in the chat interface
2. The message is sent to GPT-4o-mini along with the conversation history and database schema
3. The AI determines if a database query is needed and generates the appropriate SQL
4. The `get_data_df` tool executes the SQL query against the database
5. Results are formatted as a pandas DataFrame and displayed to the user
6. The AI provides a natural language response based on the data

## ğŸ“š Learning Resources

This repository is designed as a comprehensive learning resource for building AI-powered data agents:

### 1. Developer Guide (`Developer_Guide.ipynb`)

A step-by-step Jupyter notebook that teaches you how to build the entire application:
- Setting up the OpenAI API and understanding API parameters
- Creating effective system prompts with database schemas
- Implementing function calling and tool use
- Building database connection utilities
- Developing the Streamlit chat interface
- Managing conversation state and history

### 2. Cloud Database Guide (`Cloud_DB_Guide.ipynb`)

Learn how to:
- Set up a cloud PostgreSQL database (Neon, AWS, GCP)
- Migrate the Sakila schema to PostgreSQL
- Import data into your cloud database
- Configure secure connections
- Modify the agent to work with cloud databases

### 3. Structured Output Guide (`Structured_Output_Guide.ipynb`)

Advanced topics including:
- Using OpenAI's structured output feature
- Implementing Pydantic models for response validation
- Type-safe AI responses
- Advanced prompt engineering techniques

### 4. Development Process

- Start with `app.py` - it's intentionally minimal for you to build up
- Follow the Developer Guide step by step
- Reference `app_solution.py` only after attempting to build it yourself
- Experiment with different prompts and queries
- Try extending the agent with additional tools and capabilities

### 5. Learning Goals

By working through this project, you'll learn:
- âœ… How to integrate OpenAI's API into Python applications
- âœ… Implementing AI function calling for tool use
- âœ… Building conversational AI agents
- âœ… Natural language to SQL query generation
- âœ… Database connectivity and query execution
- âœ… Creating interactive web applications with Streamlit
- âœ… Managing application state and conversation history
- âœ… Best practices for AI prompt engineering

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.
